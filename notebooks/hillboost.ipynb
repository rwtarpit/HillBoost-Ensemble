{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91716,"databundleVersionId":11893428,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":11663414,"sourceType":"datasetVersion","datasetId":7319729},{"sourceId":11899564,"sourceType":"datasetVersion","datasetId":7480207}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import *\nfrom catboost import Pool, CatBoostClassifier, CatBoostRegressor\nimport cupy as cp, gc\nimport cudf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, optimizers, callbacks\nfrom tensorflow.keras.models import Sequential\nimport tensorflow as tf\nfrom sklearn.preprocessing import StandardScaler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T14:19:04.163353Z","iopub.execute_input":"2025-06-04T14:19:04.163701Z","iopub.status.idle":"2025-06-04T14:19:29.047209Z","shell.execute_reply.started":"2025-06-04T14:19:04.163654Z","shell.execute_reply":"2025-06-04T14:19:29.046673Z"}},"outputs":[{"name":"stderr","text":"2025-06-04 14:19:16.729271: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749046756.948475      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749046757.011813      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import KFold\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, optimizers, callbacks\nfrom tensorflow.keras.models import Sequential\nimport tensorflow as tf\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\nimport warnings\nwarnings.filterwarnings('ignore')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T14:19:29.047912Z","iopub.execute_input":"2025-06-04T14:19:29.048604Z","iopub.status.idle":"2025-06-04T14:19:29.053276Z","shell.execute_reply.started":"2025-06-04T14:19:29.048580Z","shell.execute_reply":"2025-06-04T14:19:29.052579Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nimport pandas as pd\nimport numpy as np\n\nclass MathFeatureCreator(BaseEstimator, TransformerMixin):\n    def __init__(self, variables=None, operations=None):\n        self.variables = variables\n        self.operations = operations or ['add','sub','mul', 'div']\n    \n    def fit(self, X, y=None):\n        return self  # Nothing to fit\n    \n    def transform(self, X):\n        X = X.copy()\n        new_features = []  # This will hold the new columns to concat at once\n\n        for i in range(len(self.variables)):\n            for j in range(i + 1, len(self.variables)):\n                var1 = self.variables[i]\n                var2 = self.variables[j]\n                \n                if 'add' in self.operations:\n                    new_features.append(X[var1] + X[var2])\n                if 'sub' in self.operations:\n                    new_features.append(X[var1] - X[var2])\n                if 'mul' in self.operations:\n                    new_features.append(X[var1] * X[var2])\n                if 'div' in self.operations:\n                    new_features.append(X[var1] / X[var2].replace(0, np.nan))  # Handle div by 0\n                \n\n        # Concatenate all new features at once\n        new_features_df = pd.concat(new_features, axis=1)\n        \n        # Rename columns\n        new_feature_names = [\n            f'{self.variables[i]}_{op}_{self.variables[j]}'\n            for i in range(len(self.variables))\n            for j in range(i + 1, len(self.variables))\n            for op in self.operations\n        ]\n        new_features_df.columns = new_feature_names\n        \n        # Concatenate the original dataframe with the new features\n        X = pd.concat([X, new_features_df], axis=1)\n        \n        return X","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T14:19:29.055539Z","iopub.execute_input":"2025-06-04T14:19:29.056031Z","iopub.status.idle":"2025-06-04T14:19:29.091869Z","shell.execute_reply.started":"2025-06-04T14:19:29.056003Z","shell.execute_reply":"2025-06-04T14:19:29.091327Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nclass ExponentFeatureCreator(BaseEstimator, TransformerMixin):\n    def __init__(self, operations=None, features=None):\n        self.operations = operations or ['sqrt','cbrt']\n        self.variables = features\n        \n    def fit(self, X):\n        return self\n\n    def transform(self, X):\n        X = X.copy()\n        new_feature_dict = {}\n\n        for var in self.variables:\n            if 'sqrt' in self.operations:\n                new_feature_dict[f\"{var}_sqrt\"] = np.sqrt(X[var])\n            if 'cbrt' in self.operations:\n                new_feature_dict[f\"{var}_cbrt\"] = np.cbrt(X[var])\n            if 'log' in self.operations:\n                new_feature_dict[f\"{var}_log\"] = np.log1p(X[var])\n            if 'inverse' in self.operations:\n                new_feature_dict[f\"{var}_inverse\"] = np.where(X[var] != 0, 1 / X[var], np.nan)\n            if 'exp' in self.operations:\n                new_feature_dict[f\"{var}_exp\"] = np.exp(X[var])\n\n        # Concatenate new features to original DataFrame\n        new_features_df = pd.DataFrame(new_feature_dict, index=X.index)\n        X = pd.concat([X, new_features_df], axis=1)\n\n        return X\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T14:19:30.749205Z","iopub.execute_input":"2025-06-04T14:19:30.749472Z","iopub.status.idle":"2025-06-04T14:19:30.765306Z","shell.execute_reply.started":"2025-06-04T14:19:30.749453Z","shell.execute_reply":"2025-06-04T14:19:30.764771Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"\ndef create_keras_model(input_dim, params, problem_type='regression'):\n    \"\"\"Create a Keras model according to specified parameters\"\"\"\n    \n    # Extract parameters for model architecture\n    hidden_layers = params.get('hidden_layer_sizes', (64, 32))\n    activation = params.get('activation', 'relu')\n    learning_rate = params.get('learning_rate_init', 0.001)\n    \n    # Handle dropout rates (can be a single float or a list for per-layer dropout)\n    dropout_rates = params.get('dropout_rate', 0.2)\n    if not isinstance(dropout_rates, (list, tuple)):\n        # If a single value is provided, use it for all layers\n        dropout_rates = [dropout_rates] * len(hidden_layers)\n    elif len(dropout_rates) < len(hidden_layers):\n        # If list is too short, extend it with the last value\n        dropout_rates = list(dropout_rates) + [dropout_rates[-1]] * (len(hidden_layers) - len(dropout_rates))\n    \n    # Handle activations (can be a single string or a list for per-layer activation)\n    activations = params.get('activation', 'relu')\n    if not isinstance(activations, (list, tuple)):\n        # If a single value is provided, use it for all layers\n        activations = [activations] * len(hidden_layers)\n    elif len(activations) < len(hidden_layers):\n        # If list is too short, extend it with the last value\n        activations = list(activations) + [activations[-1]] * (len(hidden_layers) - len(activations))\n    \n    use_batch_norm = params.get('batch_normalization', False)\n    l1_reg = params.get('l1_reg', 0.0)\n    l2_reg = params.get('l2_reg', 0.0)\n    kernel_regularizer = None\n    if l1_reg > 0 or l2_reg > 0:\n        kernel_regularizer = keras.regularizers.l1_l2(l1=l1_reg, l2=l2_reg)\n    \n    # Define model\n    model = Sequential()\n    \n    # Input layer\n    model.add(layers.Input(shape=(input_dim,)))\n    \n    # Hidden layers\n    for i, units in enumerate(hidden_layers):\n        model.add(layers.Dense(\n            units, \n            activation=None,  # Apply activation after batch norm if used\n            kernel_regularizer=kernel_regularizer,\n            kernel_initializer=params.get('kernel_initializer', 'glorot_uniform')\n        ))\n        \n        if use_batch_norm:\n            model.add(layers.BatchNormalization())\n            \n        model.add(layers.Activation(activations[i]))\n        \n        if dropout_rates[i] > 0:\n            model.add(layers.Dropout(dropout_rates[i]))\n    \n    # Output layer\n    if problem_type == 'regression':\n        model.add(layers.Dense(1, activation='linear'))\n    else:  # Binary classification\n        model.add(layers.Dense(1, activation='sigmoid'))\n    \n    # Compile the model\n    optimizer_name = params.get('solver', 'adam').lower()\n    if optimizer_name == 'adam':\n        optimizer = optimizers.Adam(learning_rate=learning_rate)\n    elif optimizer_name == 'sgd':\n        optimizer = optimizers.SGD(learning_rate=learning_rate, momentum=params.get('momentum', 0.9))\n    elif optimizer_name == 'rmsprop':\n        optimizer = optimizers.RMSprop(learning_rate=learning_rate)\n    elif optimizer_name == 'adagrad':\n        optimizer = optimizers.Adagrad(learning_rate=learning_rate)\n    else:\n        raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\n    \n    if problem_type == 'regression':\n        model.compile(\n            optimizer=optimizer,\n            loss=params.get('loss', 'mean_squared_error'),\n        )\n    else:  # Binary classification\n        model.compile(\n            optimizer=optimizer,\n            loss=params.get('loss', 'binary_crossentropy'),\n        )\n    \n    return model\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T14:19:30.831232Z","iopub.execute_input":"2025-06-04T14:19:30.831486Z","iopub.status.idle":"2025-06-04T14:19:30.852440Z","shell.execute_reply.started":"2025-06-04T14:19:30.831467Z","shell.execute_reply":"2025-06-04T14:19:30.851704Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def keras_ann_model(train, test, TARGET, params, folds=5, return_proba=True):\n    \n    # Define classification and regression metrics\n    classification_metrics = ['accuracy', 'f1', 'precision', 'recall', 'auc']\n    regression_metrics = ['rmse', 'mae', 'r2']\n    \n    # Extract and prepare parameters\n    metric = params.pop('eval_metric', 'rmse')\n    \n    # Determine problem type based on evaluation metric\n    problem_type = 'classification' if metric.lower() in classification_metrics else 'regression'\n    \n    random_state = params.get('random_state', 42)\n    batch_size = params.get('batch_size', 32)\n    epochs = params.get('max_iter', 100)\n    patience = params.get('patience', 3)\n    verbose = params.get('verbose', 1)\n    \n    # Set seed for reproducibility\n    tf.random.set_seed(random_state)\n    np.random.seed(random_state)\n    \n    # Prepare data\n    FOLDS = folds\n    oof_preds = np.zeros(len(train))\n    test_preds = np.zeros(len(test))\n    FEATURES = [col for col in train.columns if col != TARGET]\n    \n    # Reset indices for consistent fold splitting\n    train = train.reset_index(drop=True)\n    test = test.reset_index(drop=True)\n\n    scaler = StandardScaler()\n    train[FEATURES] = scaler.fit_transform(train[FEATURES])\n    test[FEATURES] = scaler.transform(test[FEATURES])\n\n    \n    # Prepare cross-validation\n    kf = KFold(n_splits=FOLDS, shuffle=True, random_state=random_state)\n    \n    print(f' Training Keras ANN for {problem_type}...')\n    \n    # Train across folds\n    for i, (train_idx, val_idx) in enumerate(kf.split(train)):\n        print(f' Fold {i + 1}/{FOLDS}')\n        \n        # Prepare training and validation sets\n        X_train = train.loc[train_idx, FEATURES].values\n        y_train = train.loc[train_idx, TARGET].values\n        X_val = train.loc[val_idx, FEATURES].values\n        y_val = train.loc[val_idx, TARGET].values\n        X_test = test[FEATURES].values\n        \n        # Create and train model\n        model = create_keras_model(\n            input_dim=len(FEATURES), \n            params=params, \n            problem_type=problem_type\n        )\n        \n        # Prepare callbacks\n        callbacks_list = [\n            callbacks.EarlyStopping(\n                monitor='val_loss',\n                patience=patience,\n                restore_best_weights=True\n            )\n        ]\n        \n        # Fit the model\n        model.fit(\n            X_train, y_train,\n            validation_data=(X_val, y_val),\n            epochs=epochs,\n            batch_size=batch_size,\n            callbacks=callbacks_list,\n            verbose=verbose\n        )\n        \n        # Make predictions\n        val_preds = model.predict(X_val, verbose=0).flatten()\n        test_fold_preds = model.predict(X_test, verbose=0).flatten()\n        \n        # Store predictions\n        oof_preds[val_idx] = val_preds\n        test_preds += test_fold_preds / FOLDS\n        \n        # Evaluate fold performance\n        fold_score = get_metric(metric, y_val, val_preds)\n        print(f' Fold {metric.upper()}: {fold_score:.5f}')\n    \n    # Evaluate overall performance\n    overall_score = get_metric(metric, train[TARGET], oof_preds)\n    print(f'Overall OOF {metric.upper()}: {overall_score:.5f}')\n    \n    # Clear Keras backend session\n    keras.backend.clear_session()\n    \n    return oof_preds, test_preds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T14:19:30.853154Z","iopub.execute_input":"2025-06-04T14:19:30.853427Z","iopub.status.idle":"2025-06-04T14:19:30.872565Z","shell.execute_reply.started":"2025-06-04T14:19:30.853406Z","shell.execute_reply":"2025-06-04T14:19:30.871835Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.model_selection import KFold\nimport numpy as np\n\ndef xgb_model(train, test, TARGET, params, folds):\n    params['device'] = params.get('device', 'cuda')\n    FOLDS = folds\n    oof_xgb = np.zeros(len(train))\n    pred_xgb = np.zeros(len(test))\n    FEATURES = [col for col in train if col != TARGET]\n    train = train.reset_index(drop=True)\n    test = test.reset_index(drop=True)\n\n    kf = KFold(n_splits=FOLDS, shuffle=True, random_state=42)\n    print('Training XGBoost model...')\n\n    for i, (train_index, val_index) in enumerate(kf.split(train)):\n        print(f'Fold {i + 1}')\n        X_train = train.loc[train_index, FEATURES]\n        y_train_fold = train.loc[train_index, TARGET]\n        X_val = train.loc[val_index, FEATURES]\n        y_val = train.loc[val_index, TARGET]\n        X_test = test[FEATURES]\n\n        dtrain = xgb.DMatrix(X_train, label=y_train_fold)\n        dval = xgb.DMatrix(X_val, label=y_val)\n        dtest = xgb.DMatrix(X_test)\n\n        model_xgb = xgb.train(\n            params=params,\n            dtrain=dtrain,\n            num_boost_round=10000,\n            evals=[(dval, 'validation')],\n            early_stopping_rounds=3,\n            verbose_eval=False \n        )\n\n        # After training, print validation results (from last iteration)\n        print(f\"Validation results after Fold {i + 1}:\")\n        print(f\"Best iteration: {model_xgb.best_iteration}, Best score: {model_xgb.best_score}\")\n\n        val_preds = model_xgb.predict(dval)\n        oof_xgb[val_index] = val_preds\n        pred_xgb += model_xgb.predict(dtest)\n\n    pred_xgb /= FOLDS\n\n    return oof_xgb, pred_xgb\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T14:19:30.873320Z","iopub.execute_input":"2025-06-04T14:19:30.873552Z","iopub.status.idle":"2025-06-04T14:19:30.886456Z","shell.execute_reply.started":"2025-06-04T14:19:30.873537Z","shell.execute_reply":"2025-06-04T14:19:30.885874Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def lgb_model(train, test, TARGET, params, folds):\n    params['verbosity'] = params.get('verbosity', -1)\n    params['device'] = params.get('device', 'gpu')\n    FOLDS = folds\n    oof_lgb = np.zeros(len(train))\n    pred_lgb = np.zeros(len(test))\n    FEATURES = [col for col in train if col != TARGET]\n    train = train.reset_index(drop=True)\n    test = test.reset_index(drop=True)\n\n    kf = KFold(n_splits=FOLDS, shuffle=True, random_state=42)\n    print('Training LightGBM model...')\n\n    for i, (train_index, val_index) in enumerate(kf.split(train)):\n        print(f'Fold {i + 1}')\n        X_train = train.loc[train_index, FEATURES]\n        y_train_fold = train.loc[train_index, TARGET]\n        X_val = train.loc[val_index, FEATURES]\n        y_val = train.loc[val_index, TARGET]\n        X_test = test[FEATURES]\n\n        dtrain = lgb.Dataset(X_train, label=y_train_fold)\n        dval = lgb.Dataset(X_val, label=y_val)\n\n        model_lgb = lgb.train(\n            params=params,\n            train_set=dtrain,\n            num_boost_round=10000,\n            valid_sets=[dval],\n             callbacks=[\n                lgb.early_stopping(stopping_rounds=3),\n                lgb.log_evaluation(0)]  \n    \n        )\n\n        val_preds = model_lgb.predict(X_val, num_iteration=model_lgb.best_iteration)\n        oof_lgb[val_index] = val_preds\n        pred_lgb += model_lgb.predict(X_test, num_iteration=model_lgb.best_iteration)\n\n    pred_lgb /= FOLDS\n\n    return oof_lgb, pred_lgb\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T14:19:30.887265Z","iopub.execute_input":"2025-06-04T14:19:30.887492Z","iopub.status.idle":"2025-06-04T14:19:30.906031Z","shell.execute_reply.started":"2025-06-04T14:19:30.887477Z","shell.execute_reply":"2025-06-04T14:19:30.905415Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def catboost_model(train, test, TARGET, params, folds):\n    # Set default parameters for verbosity and iterations if not provided\n    params['verbose'] = params.get('verbose', False)\n    params['task_type'] = params.get('task_type', 'GPU')\n    params['iterations'] = params.get('iterations', 10000)\n    FOLDS = folds\n    oof_cb = np.zeros(len(train))  # Out-of-Fold predictions\n    pred_cb = np.zeros(len(test))  # Test predictions\n    FEATURES = [col for col in train if col != TARGET]\n    train = train.reset_index(drop=True)\n    test = test.reset_index(drop=True)\n\n    # Determine if the task is classification or regression based on params\n    is_classification = 'Logloss' in params['loss_function'] or 'CrossEntropy' in params['loss_function']\n\n    kf = KFold(n_splits=FOLDS, shuffle=True, random_state=42)\n    print('Training CatBoost model...')\n\n    for i, (train_index, val_index) in enumerate(kf.split(train)):\n        print(f'Fold {i + 1}')\n        X_train = train.loc[train_index, FEATURES]\n        y_train_fold = train.loc[train_index, TARGET]\n        X_val = train.loc[val_index, FEATURES]\n        y_val = train.loc[val_index, TARGET]\n        X_test = test[FEATURES]\n\n        dtrain = Pool(data=X_train, label=y_train_fold)\n        dval = Pool(data=X_val, label=y_val)\n\n        # Choose the correct CatBoost model based on classification or regression\n        if is_classification:\n            model_cb = CatBoostClassifier(**params)\n        else:\n            model_cb = CatBoostRegressor(**params)\n\n        # Train the model\n        model_cb.fit(\n            dtrain,\n            eval_set=dval,\n            early_stopping_rounds=3,\n            verbose=False\n        )\n\n        # Get predictions\n        if is_classification:\n            val_preds = model_cb.predict_proba(X_val)[:, 1]  # Probabilities for the positive class\n            pred_cb += model_cb.predict_proba(X_test)[:, 1]\n        else:\n            val_preds = model_cb.predict(X_val)  # Continuous predictions for regression\n            pred_cb += model_cb.predict(X_test)\n\n        # Calculate validation score\n        best_iter = model_cb.get_best_iteration()\n        best_score = model_cb.get_best_score()\n        val_score = best_score['validation'][model_cb.get_param('eval_metric')]\n\n        print(f'Best Iteration: {best_iter} | Validation {model_cb.get_param(\"eval_metric\")}: {val_score:.5f}')\n\n        # Store the predictions\n        oof_cb[val_index] = val_preds\n\n    # Average predictions for the test set\n    pred_cb /= FOLDS\n\n    return oof_cb, pred_cb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T14:19:30.906726Z","iopub.execute_input":"2025-06-04T14:19:30.907004Z","iopub.status.idle":"2025-06-04T14:19:30.924264Z","shell.execute_reply.started":"2025-06-04T14:19:30.906980Z","shell.execute_reply":"2025-06-04T14:19:30.923725Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def weighted_mean_absolute_error(y_true, y_pred, weights):\n        return np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)\n\ndef rmsLe(y_true, y_pred):\n    y_pred = np.maximum(y_pred, 1e-6)\n    return np.sqrt(mean_squared_log_error(y_true, y_pred))\n\ndef mape(y_true, y_pred):\n    return mean_absolute_percentage_error(y_true, y_pred)\n\ndef logloss(y_true, y_prob, eps=1e-15):\n        y_prob = np.clip(y_prob, eps, 1 - eps)  # Avoid log(0)\n        return -np.mean(y_true * np.log(y_prob) + (1 - y_true) * np.log(1 - y_prob))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T14:19:30.924866Z","iopub.execute_input":"2025-06-04T14:19:30.925017Z","iopub.status.idle":"2025-06-04T14:19:30.941307Z","shell.execute_reply.started":"2025-06-04T14:19:30.925005Z","shell.execute_reply":"2025-06-04T14:19:30.940744Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def get_metric(metric, y_true, y_pred, num_classes=None, weights=None, custom_metric=None):\n    if metric == 'roc_auc':\n        return roc_auc_score(y_true, y_pred, multi_class=\"ovr\" if num_classes and num_classes > 2 else None)\n    elif metric == 'accuracy':\n        return accuracy_score(y_true, y_pred.round())\n    elif metric == 'f1':\n        return f1_score(y_true, y_pred.round(), average='weighted') if num_classes and num_classes > 2 else f1_score(y_true, y_pred.round())\n    elif metric == 'precision':\n        return precision_score(y_true, y_pred.round(), average='weighted') if num_classes and num_classes > 2 else precision_score(y_true, y_pred.round())\n    elif metric == 'recall':\n        return recall_score(y_true, y_pred.round(), average='weighted') if num_classes and num_classes > 2 else recall_score(y_true, y_pred.round())\n    elif metric == 'mae':\n        return mean_absolute_error(y_true, y_pred)\n    elif metric == 'r2':\n        return r2_score(y_true, y_pred)\n    elif metric == 'rmse':\n        return mean_squared_error(y_true, y_pred, squared=False)\n    elif metric == 'wmae' and weights is not None:\n        return weighted_mean_absolute_error(y_true, y_pred, weights)  # You must define this externally\n    elif metric == 'rmsle':\n        return rmsle(y_true, y_pred) \n    elif metric == 'mse':\n        return mean_squared_error(y_true, y_pred, squared=True)\n    elif metric == \"mape\":\n        return mape(y_true, y_pred)\n    elif metric == 'logloss':\n        return logloss(y_true,y_pred)\n    elif metric == 'custom' and callable(custom_metric):\n        return custom_metric(y_true, y_pred)\n    else:\n        raise ValueError(f\"Unsupported metric '{metric}'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T14:19:30.941920Z","iopub.execute_input":"2025-06-04T14:19:30.942179Z","iopub.status.idle":"2025-06-04T14:19:30.959777Z","shell.execute_reply.started":"2025-06-04T14:19:30.942162Z","shell.execute_reply":"2025-06-04T14:19:30.959140Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"import cupy as cp\n\ndef get_metric_gpu(metric, y_true, y_pred, num_classes=None, weights=None, custom_metric=None):\n    if isinstance(y_true, pd.Series):\n        y_true = cp.array(y_true.values)\n    \n    if len(y_true.shape) == 1:\n        y_true = y_true[:, cp.newaxis]\n\n    if isinstance(y_pred, pd.Series):\n        y_pred = cp.array(y_pred.values)\n    \n    if len(y_pred.shape) == 1:\n        y_pred = y_pred[:, cp.newaxis] \n        \n    def accuracy_score_gpu(y_true, y_pred):\n        return cp.mean(y_true == y_pred)\n\n    def precision_score_gpu(y_true, y_pred):\n        if num_classes and num_classes > 2:\n            scores, weights_ = [], []\n            for cls in range(num_classes):\n                tp = cp.sum((y_true == cls) & (y_pred == cls))\n                fp = cp.sum((y_true != cls) & (y_pred == cls))\n                score = tp / (tp + fp + 1e-8)\n                scores.append(score)\n                weights_.append(cp.sum(y_true == cls))\n            return cp.sum(cp.array(scores) * cp.array(weights_)) / (cp.sum(weights_) + 1e-8)\n        else:\n            tp = cp.sum((y_true == 1) & (y_pred == 1))\n            fp = cp.sum((y_true == 0) & (y_pred == 1))\n            return tp / (tp + fp + 1e-8)\n\n    def recall_score_gpu(y_true, y_pred):\n        if num_classes and num_classes > 2:\n            scores, weights_ = [], []\n            for cls in range(num_classes):\n                tp = cp.sum((y_true == cls) & (y_pred == cls))\n                fn = cp.sum((y_true == cls) & (y_pred != cls))\n                score = tp / (tp + fn + 1e-8)\n                scores.append(score)\n                weights_.append(cp.sum(y_true == cls))\n            return cp.sum(cp.array(scores) * cp.array(weights_)) / (cp.sum(weights_) + 1e-8)\n        else:\n            tp = cp.sum((y_true == 1) & (y_pred == 1))\n            fn = cp.sum((y_true == 1) & (y_pred == 0))\n            return tp / (tp + fn + 1e-8)\n\n    def f1_score_gpu(y_true, y_pred):\n        precision = precision_score_gpu(y_true, y_pred)\n        recall = recall_score_gpu(y_true, y_pred)\n        return 2 * precision * recall / (precision + recall + 1e-8)\n\n    def mae_gpu(y_true, y_pred):\n        return cp.mean(cp.abs(y_true - y_pred))\n\n    def mse_gpu(y_true, y_pred):\n        return cp.mean((y_true - y_pred) ** 2)\n\n    def rmse_gpu(y_true, y_pred):\n        return cp.sqrt(mse_gpu(y_true, y_pred))\n\n    def rmsle_gpu(y_true, y_pred):\n        y_true_log = cp.log1p(cp.clip(y_true, a_min=0, a_max=None))\n        y_pred_log = cp.log1p(cp.clip(y_pred, a_min=0, a_max=None))\n        return cp.sqrt(cp.mean((y_true_log - y_pred_log) ** 2))\n\n    def r2_score_gpu(y_true, y_pred):\n        ss_res = cp.sum((y_true - y_pred) ** 2)\n        ss_tot = cp.sum((y_true - cp.mean(y_true)) ** 2)\n        return 1 - ss_res / (ss_tot + 1e-8)\n\n    def mape_gpu(y_true, y_pred):\n        return cp.mean(cp.abs((y_true - y_pred) / cp.clip(y_true, 1e-8, None))) * 100\n\n    def weighted_mae_gpu(y_true, y_pred):\n        return cp.sum(cp.abs(y_true - y_pred) * weights) / (cp.sum(weights) + 1e-8)\n\n    def roc_auc_score_gpu(y_true, y_pred):\n    # Sort scores and corresponding true values\n        desc_score_indices = cp.argsort(-y_pred)\n        y_true_sorted = y_true[desc_score_indices]\n        \n        # Count positives and negatives\n        n_pos = cp.sum(y_true_sorted == 1)\n        n_neg = cp.sum(y_true_sorted == 0)\n        \n        # Cumulative sum of positives\n        tps = cp.cumsum(y_true_sorted == 1)\n        fps = cp.cumsum(y_true_sorted == 0)\n        \n        # Avoid division by zero\n        if n_pos == 0 or n_neg == 0:\n            return cp.nan  # Undefined AUC\n    \n        # Calculate AUC using trapezoidal rule\n        fpr = fps / n_neg\n        tpr = tps / n_pos\n        auc = cp.trapz(tpr, fpr)\n        return auc\n    def logloss_gpu(y_true, y_prob, eps=1e-15):\n        y_prob = cp.clip(y_prob, eps, 1 - eps)  # Avoid log(0)\n        return -cp.mean(y_true * cp.log(y_prob) + (1 - y_true) * cp.log(1 - y_prob))\n\n\n    if metric == 'accuracy':\n        return accuracy_score_gpu(y_true, y_pred)\n    elif metric == 'roc_auc':\n        return roc_auc_score_gpu(y_true, y_pred)\n    elif metric == 'precision':\n        return precision_score_gpu(y_true, y_pred)\n    elif metric == 'recall':\n        return recall_score_gpu(y_true, y_pred)\n    elif metric == 'f1':\n        return f1_score_gpu(y_true, y_pred)\n    elif metric == 'mae':\n        return mae_gpu(y_true, y_pred)\n    elif metric == 'mse':\n        return mse_gpu(y_true, y_pred)\n    elif metric == 'rmse':\n        return rmse_gpu(y_true, y_pred)\n    elif metric == 'rmsle':\n        return rmsle_gpu(y_true, y_pred)\n    elif metric == 'r2':\n        return r2_score_gpu(y_true, y_pred)\n    elif metric == 'mape':\n        return mape_gpu(y_true, y_pred)\n    elif metric == 'wmae' and weights is not None:\n        return weighted_mae_gpu(y_true, y_pred)\n    elif metric == 'custom' and callable(custom_metric):\n        return custom_metric(y_true, y_pred)\n    elif metric == 'logloss':\n        return logloss_gpu(y_true,y_pred)\n        \n    else:\n        raise ValueError(f\"Unsupported metric '{metric}'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T14:19:30.960445Z","iopub.execute_input":"2025-06-04T14:19:30.960698Z","iopub.status.idle":"2025-06-04T14:19:30.982507Z","shell.execute_reply.started":"2025-06-04T14:19:30.960677Z","shell.execute_reply":"2025-06-04T14:19:30.982000Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"class HillBoost():\n    def __init__(self, models, folds, hc_metric):\n        if not isinstance(models, dict):\n            raise TypeError(\"'models' must be a dict with keys as model_name & values as another dict for parameters\")\n\n        self.models = models\n        self.hc_metric = hc_metric\n        self.folds = folds\n\n    def hill_climb_df(self, train, test, target):\n        FEATURES = [col for col in train.columns if col != target]\n        oof_df = pd.DataFrame()\n        pred_df = pd.DataFrame()\n\n        for idx, (model, params) in enumerate(self.models.items()):\n            print(f'Model : {model} is on the run..')\n            if model.startswith('xgboost'):\n                model_oof, model_pred = xgb_model(train, test, target, params, self.folds)\n                oof_df[f'{model}'] = model_oof\n                pred_df[f'{model}'] = model_pred\n\n            if model.startswith('lightgbm'):\n                model_oof, model_pred = lgb_model(train, test, target, params, self.folds)\n                oof_df[f'{model}'] = model_oof\n                pred_df[f'{model}'] = model_pred\n\n            if model.startswith('catboost'):\n                model_oof, model_pred = catboost_model(train, test, target, params, self.folds)\n                oof_df[f'{model}'] = model_oof\n                pred_df[f'{model}'] = model_pred\n\n            if model.startswith(('mlp','ann')):\n                model_oof, model_pred = keras_ann_model(train, test, target, params, self.folds)\n                oof_df[f'{model}'] = model_oof\n                pred_df[f'{model}'] = model_pred\n                \n        return oof_df, pred_df\n\n    def HillClimb(self,train, test, target, iterations=10, min_improvement=1e-7):\n        \"\"\"\n        Enhanced hill climbing algorithm for ensemble building\n        \n        Parameters:\n        - train: Training dataframe with OOF predictions\n        - test: Test dataframe with predictions\n        - target: Target column name or array\n        - iterations: Number of passes through the model set (default: 10)\n        - min_improvement: Minimum improvement threshold (default: 1e-7)\n        \"\"\"\n        import pandas as pd\n        import numpy as np\n        import cupy as cp  # Using GPU acceleration\n        import time\n        \n        # Define metrics that should be maximized or minimized\n        maximize_metrics = [\n            'roc_auc',\n            'accuracy',\n            'f1',\n            'precision',\n            'recall',\n            'r2'\n        ]\n        \n        minimize_metrics = [\n            'mae',\n            'rmse',\n            'wmae',\n            'rmsle',\n            'mse',\n            'mape',\n            'logloss'\n        ]\n    \n        oof_df,pred_df=self.hill_climb_df(train,test,target)\n        # Algorithm parameters - adjusted for better performance\n        USE_NEGATIVE_WGT = True       # Allow negative weights for potentially better combinations\n        MAX_MODELS = 20               # Maximum number of models in ensemble\n        TOL = min_improvement         # Use a smaller tolerance for more precise optimization\n        metric = self.hc_metric              # Metric to optimize\n        \n        \n        # Handle target as column name or direct array\n        if isinstance(target, str):\n            TARGET = train[target].values\n            # Remove target column from predictions if it's there\n            if target in oof_df.columns:\n                oof_df = oof_df.drop(columns=[target])\n            if target in pred_df.columns:\n                pred_df = pred_df.drop(columns=[target])\n        else:\n            TARGET = target\n        \n        model_names = oof_df.columns\n        print(f\"Working with {len(model_names)} models: {', '.join(model_names)}\")\n        \n        # Find the best individual model\n        best_score = float('inf') if metric in minimize_metrics else -float('inf')\n        best_index = -1\n        \n        for k, name in enumerate(model_names):\n            metric_calculated = get_metric(metric, TARGET, oof_df.iloc[:, k])\n            if metric in minimize_metrics:\n                if metric_calculated < best_score:\n                    best_score = metric_calculated\n                    best_index = k\n            else:  # maximize metric\n                if metric_calculated > best_score:\n                    best_score = metric_calculated\n                    best_index = k\n            print(f'{metric} {metric_calculated:0.5f} {name}')\n    \n        print(f'Best single model is {model_names[best_index]} with {metric} = {best_score:0.5f}')\n        \n        # Initialize the ensemble with the best model\n        indices = [best_index]\n        old_best_score = best_score\n        print(f'0 We begin with best single model {metric} {best_score:0.5f} from \"{model_names[best_index]}\"')\n        \n        # Prepare variables for GPU processing\n        x_train2 = cp.array(oof_df.values)  # GPU\n        x_train3 = cp.array(pred_df.values)  # GPU\n        best_ensemble = x_train2[:, best_index].copy()  # GPU - explicitly copy to avoid reference issues\n        truth = cp.array(TARGET)  # GPU\n        \n        # Improved weight range with finer granularity\n        start = -0.60 if USE_NEGATIVE_WGT else 0.01\n        # Use even finer weight steps for more precise optimization\n        ww = cp.concatenate([\n            cp.arange(start, -0.01, 0.05),     # Coarse steps for negative weights\n            cp.arange(-0.01, 0.01, 0.001),     # Very fine steps around zero\n            cp.arange(0.01, 0.11, 0.005),      # Fine steps for small weights\n            cp.arange(0.11, 1.01, 0.01)        # Regular steps for larger weights\n        ])\n        nn = len(ww)\n        print(f\"Testing {nn} different weight combinations\")\n        \n        # Begin hill climbing with multiple iterations\n        new_rows = []\n        final_pred = x_train3[:, best_index].copy()  # Explicit copy\n        ensemble_weights = {model_names[best_index]: 1.0}  # Track all model weights\n        \n        improvement_found = False\n        \n        for iteration in range(iterations):\n            print(f\"\\nIteration {iteration+1}/{iterations}\")\n            models_added = 0\n            \n            # Shuffle the model order for more exploration in each iteration\n            model_indices = list(range(len(model_names)))\n            if iteration > 0:  # Only shuffle after first iteration to keep logging consistent\n                np.random.shuffle(model_indices)\n            \n            # Try each model that hasn't been added yet\n            for idx in model_indices:\n                model = model_names[idx]\n                \n                if len(indices) >= MAX_MODELS:\n                    print(f\"Reached maximum number of models ({MAX_MODELS})\")\n                    break\n                    \n                # Skip the best model in first iteration (it's already included)\n                if iteration == 0 and idx == best_index:\n                    continue\n                \n                # Create potential ensembles with different weights\n                new_model = x_train2[:, idx]  # GPU\n                m1 = cp.repeat(best_ensemble[:, cp.newaxis], nn, axis=1) * (1-ww)  # GPU\n                m2 = cp.repeat(new_model[:, cp.newaxis], nn, axis=1) * ww  # GPU\n                mm = m1 + m2  # GPU\n    \n                new_metrics = get_metric_gpu(metric, truth, mm)\n    \n                if metric in minimize_metrics:\n                    new_score = cp.min(new_metrics).item()\n                    new_idx = cp.argmin(new_metrics).item()\n                    is_better = new_score < old_best_score\n                    diff = old_best_score - new_score\n                else:  # maximize metrics\n                    new_score = cp.max(new_metrics).item()\n                    new_idx = cp.argmax(new_metrics).item()\n                    is_better = new_score > old_best_score\n                    diff = new_score - old_best_score\n                    \n                if is_better and abs(diff) > TOL:\n                    improvement_found = True\n                    print(f'Improvement found by {abs(diff):.6f}')\n                    best_ensemble = mm[:, new_idx].copy()  # Explicit copy\n                    best_weight = ww[new_idx].item()\n                    old_best_score = new_score\n                    \n                    # Track which models we've used (even if re-used in later iterations)\n                    if idx not in indices:\n                        indices.append(idx)\n                    \n                    # Update ensemble weights and predictions\n                    for existing_model in list(ensemble_weights.keys()):\n                        ensemble_weights[existing_model] *= (1-best_weight)\n                    \n                    # Add or update weight for the current model\n                    if model in ensemble_weights:\n                        ensemble_weights[model] += best_weight\n                    else:\n                        ensemble_weights[model] = best_weight\n                    \n                    final_pred = final_pred * (1-best_weight) + x_train3[:, idx] * best_weight\n                    new_row = {\n                        'iteration': iteration + 1,\n                        'model_added': model,\n                        'additive_weight': best_weight,\n                        'total_weight': ensemble_weights[model],\n                        'metric_value': new_score,\n                        'improvement': abs(diff)\n                    }\n                    new_rows.append(new_row)\n                    models_added += 1\n                    print(f'Added {model} with weight {best_weight:.4f} (total weight: {ensemble_weights[model]:.4f})')\n                    \n            if models_added == 0:\n                print(f\"No improvements found in iteration {iteration+1}\")\n                if iteration > 0 and not improvement_found:  # Only break if we've done at least one iteration with no improvements\n                    break\n        \n        # Calculate final score\n        if isinstance(best_ensemble, cp.ndarray):\n            best_ensemble_np = cp.asnumpy(best_ensemble)\n        else:\n            best_ensemble_np = best_ensemble\n            \n        final_new_score = get_metric(metric, TARGET, best_ensemble_np)\n        df = pd.DataFrame(new_rows)\n        \n        if df.empty:\n            print(f'No additive gain in {metric}, consider:')\n            print('1. Using more diverse models')\n            print('2. Decreasing min_improvement parameter')\n            print('3. Trying different metrics')\n            print('4. Analyzing model correlations - highly correlated models may not ensemble well')\n        else:\n            print(f'\\nEnsemble Summary:')\n            print(f'Improvement: {abs(final_new_score - best_score):.6f}')\n            print(f'Initial best score: {best_score:.6f}')\n            print(f'Final ensemble score: {final_new_score:.6f}')\n            \n            print(\"\\nFinal ensemble weights:\")\n            for model, weight in sorted(ensemble_weights.items(), key=lambda x: abs(x[1]), reverse=True):\n                if abs(weight) > 0.001:  # Only show models with non-negligible weights\n                    print(f\"  {model}: {weight:.4f}\")\n    \n        # Save predictions\n        np.save('pred_hill_climb', cp.asnumpy(final_pred) if isinstance(final_pred, cp.ndarray) else final_pred)\n        np.save('oof_hill_climb', best_ensemble_np)\n        \n        # Return the DataFrame of added models, the ensemble OOF predictions, and the test predictions\n        return df, best_ensemble_np, (cp.asnumpy(final_pred) if isinstance(final_pred, cp.ndarray) else final_pred), ensemble_weights\n    \n    # Example usage\n    # df, oof_preds, test_preds, weights = HillClimb(train, test, 'target', iterations=10, min_improvement=1e-7)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T14:22:28.609623Z","iopub.execute_input":"2025-06-04T14:22:28.610147Z","iopub.status.idle":"2025-06-04T14:22:28.632754Z","shell.execute_reply.started":"2025-06-04T14:22:28.610126Z","shell.execute_reply":"2025-06-04T14:22:28.632101Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}