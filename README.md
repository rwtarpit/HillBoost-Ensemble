# HillBoost-Ensemble

A GPU-powered ensemble learning algorithm using **hill climbing optimization** to combine models for optimal predictive performance.

> Built for speed. Tuned for performance. Inspired by chaos.

---

## üöÄ What is Hill Boost?

**Hill Boost Ensemble** finds the best-performing model on a dataset, and then **iteratively adds other models** only if they improve the overall performance. Think of it as smart ensembling with a ruthless edge.

Currently supports:

- ‚úÖ XGBoost
- ‚úÖ LightGBM
- ‚úÖ CatBoost
- ‚úÖ Keras-based MLP

> **Note:** GPU is strictly required. This repo is born and raised on the Kaggle GPU runtime.

---

## üß† How It Works

1. Train all candidate models on the same dataset
2. Evaluate performance using a custom metric (e.g., AUC, F1, accuracy)
3. Start with the best single model
4. Iteratively test all other models ‚Äî include only if ensemble improves
5. Final prediction is the weighted sum of selected models

---

## üì¶ Setup

Clone the repo:

```bash
git clone https://github.com/rwtarpit/HillBoost-Ensemble.git
cd HillBoost-Ensemble
```
## ü§ù Contributing

We welcome contributions!

Check out [`CONTRIBUTING.md`](.github/CONTRIBUTING.md) for setup steps, ideas, and good first issues to work on.


